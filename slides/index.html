<slides
  import-url="front_slide.html"
  import-place="replace"
  onerrorimport="myOnErrorImport(event)"
></slides>
<slides
  import-url="summary.html"
  import-place="replace"
  onerrorimport="myOnErrorImport(event)"
></slides>
<slides
  import-url="introduction.html"
  import-place="replace"
  onerrorimport="myOnErrorImport(event)"
></slides>
<slides
  import-url="design.html"
  import-place="replace"
  onerrorimport="myOnErrorImport(event)"
></slides>
<slides
  import-url="implementation.html"
  import-place="replace"
  onerrorimport="myOnErrorImport(event)"
></slides>
<slides
  import-url="conclusion.html"
  import-place="replace"
  onerrorimport="myOnErrorImport(event)"
></slides>
<slides
  import-url="back_slide.html"
  import-place="replace"
  onerrorimport="myOnErrorImport(event)"
></slides>

<!--
<section>
  <h4>Méthode du gradient</h4>
  <section>
    <h5>Introduction</h5>
    <ul fc>
      <li>
        Utilisable pour optimiser les problèmes non linéaires à plusieurs
        variables sans contraintes.
      </li>
      <li>Méthode itérative et approximative.</li>
    </ul>
  </section>
  <section>
    <h5>Gradient</h5>
    <ul fc>
      <!-- <li>Généralistation de la dérivée d'une fonction à plusieurs variables.</li> ->
      <li>
        Vecteur orienté vers la direction où la valeur de la fonction augmente.
      </li>
      <li>
        Si $F(\vec x)$ est une fonction de $\mathbb{R}^n$ sur $\mathbb{R}$ alors
        <br />
        $\vec\nabla F(\vec x) = \begin{pmatrix} \frac{\partial F}{\partial x_1}
        \\ \frac{\partial F}{\partial x_2} \\ \vdots \\ \frac{\partial
        F}{\partial x_n} \end{pmatrix}$.
      </li>
    </ul>
  </section>
  <section>
    <h5>Algorithme (pas fixe)</h5>
    <p f>Soit $F(\vec x)$ la fonction à optimiser.</p>
    <ol fc>
      <li>Choisir $\vec x_0$ comme solution initiale.</li>
      <li>
        Calculer $\vec x_{i+1} = \vec x_i \mp p \cdot \vec \nabla F(\vec x_i)$
        pour minimiser/maximiser $F$.
      </li>
      <li>
        Si $| \vec \nabla F(\vec x_{i+1}) | \leq \epsilon$ alors $\vec x^* =
        \vec x_{i+1}$ sinon refaire 2.
      </li>
    </ol>
  </section>
  <section>
    <h5>Algorithme (pas fixe)</h5>
    <ul fc>
      <li>
        L'inconvénient avec un pas fixe c'est que si il est petit l'algorithme
        converge lentement et si il est grand il ne converge pas.
      </li>
      <li>Problème de l'optimum local si la fonction n'est pas convexe.</li>
      <li>
        Cet algorithme est utilisé pour l'apprentissage automatique des réseaux
        de neurones.
      </li>
    </ul>
  </section>
  <section>
    <a href="gradient-fixed-step.html">Link</a>
    <sketch name="gradient-fixed-step"></sketch>
  </section>
  <section>
    <h5>Algorithme (pas variable)</h5>
    <p f>Soit $F(\vec x)$ la fonction à optimiser.</p>
    <ol fc>
      <li>Choisir $\vec x_0$ comme solution initiale.</li>
      <li>
        Trouver $p_i$ tel qu'il minimise/maximise $F(\vec x_i \mp p_i \cdot \vec
        \nabla F(\vec x_i))$.
      </li>
      <li>
        Calculer $\vec x_{i+1} = \vec x_i \mp p_i \cdot \vec \nabla F(\vec x_i)$
        pour minimiser/maximiser $F$.
      </li>
      <li>
        Si $| \vec \nabla F(\vec x_{i+1}) | \leq \epsilon$ alors $\vec x^* =
        \vec x_{i+1}$ sinon refaire 2.
      </li>
    </ol>
  </section>
  <section>
    <h5>Algorithme (pas variable)</h5>
    <ul fc>
      <li>La convergence a été amélioré.</li>
    </ul>
  </section>
  <section>
    <a href="gradient-variable-step.html">Link</a>
    <sketch name="gradient-variable-step"></sketch>
  </section>
</section>

<section>
  <h4>Méthode du gradient projeté</h4>
  <section>
    <h5>Introduction</h5>
    <ul fc>
      <li>Méthode basée sur la méthode du gradient.</li>
      <li>
        Résout les problèmes non linéaires à plusieurs variables avec
        contraintes.
      </li>
      <li>
        L'ensemble des contraintes définit une région de $\mathbb{R}^n$ (noté
        ici $K$ avec $K\subseteq \mathbb{R}^n$).
      </li>
      <li>La solution optimale doit être située dans cette région.</li>
    </ul>
  </section>
  <section>
    <h5>Définition - Convexe</h5>
    <ul fc>
      <li>
        Un ensemble est dit convexe si pour chaque deux points de cet ensemble
        le segment qu'ils délimitent reste à l'intérieur de l'ensemble.
      </li>
    </ul>
  </section>
  <section>
    <a href="constraints.html">Link</a>
    <sketch name="constraints"></sketch>
  </section>
  <section>
    <h5>Définition - Projection</h5>
    <ul fc>
      <li>
        La projection c'est une opération qui consiste à transformer un point à
        l'éxterieur d'une région en un point à l'intérieur de cette région tel
        que : <br />
        $$\textrm{proj}_K(\vec x) = \textrm{arg min}_{\vec y\in K} \;
        \textrm{dist}(\vec x, \vec y)$$
      </li>
      <li>Si $K$ est convexe $\textrm{proj}_K(\vec x)$ est toujours unique.</li>
      <li>Si $\vec x \in K$ alors $\vec x = \textrm{proj}_K(\vec x)$.</li>
    </ul>
  </section>
  <section>
    <a href="projection.html">Link</a>
    <sketch name="projection"></sketch>
  </section>
  <section>
    <h5>L'Algorithme</h5>
    <p f>
      Soit $F(\vec x)$ la fonction à optimiser et $K$ l'ensemble des
      contraintes.
    </p>
    <ol fc>
      <li>Choisir $\vec x_0$ comme solution initiale.</li>
      <li>
        Trouver $p_i$ tel qu'il minimise/maximise $F(\vec x_i \mp p_i \cdot \vec
        \nabla F(\vec x_i))$.
      </li>
      <li>
        Calculer $\vec x'_{i+1} = \vec x_i \mp p_i \cdot \vec \nabla F(\vec
        x_i)$ pour minimiser/maximiser $F$.
      </li>
      <li>Calculer $\vec x_{i+1} = \textrm{proj}_K(\vec x'_{i+1})$.</li>
      <li>
        Si $| \vec \nabla F(\vec x_{i+1}) | \leq \epsilon$ alors $\vec x^* =
        \vec x_{i+1}$ sinon refaire 2.
      </li>
    </ol>
  </section>
  <section>
    <h5>Exemple Linéaire</h5>
    <a href="linear-example.html">Link</a>
    <sketch name="linear-example"></sketch>
  </section>
  <section>
    <h5>Exemple Non-Linéaire</h5>
    <a href="projected-gradient.html">Link</a>
    <sketch name="projected-gradient"></sketch>
  </section>
  <section>
    <h5>Méthode du gradient projeté + Méthode de Monte-Carlo</h5>
    <a href="monte-carlo-projected-gradient.html">Link</a>
    <sketch name="monte-carlo-projected-gradient"></sketch>
  </section>
</section>
-->
